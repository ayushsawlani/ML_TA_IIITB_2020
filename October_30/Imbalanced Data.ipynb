{"cells":[{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:04.839516Z","iopub.status.busy":"2020-10-29T11:20:04.838653Z","iopub.status.idle":"2020-10-29T11:20:17.555210Z","shell.execute_reply":"2020-10-29T11:20:17.555896Z"},"papermill":{"duration":12.751872,"end_time":"2020-10-29T11:20:17.556100","exception":false,"start_time":"2020-10-29T11:20:04.804228","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"!pip install seaborn==0.11","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.032029,"end_time":"2020-10-29T11:20:17.623615","exception":false,"start_time":"2020-10-29T11:20:17.591586","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Breast Cancer Prediction"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:17.703202Z","iopub.status.busy":"2020-10-29T11:20:17.697569Z","iopub.status.idle":"2020-10-29T11:20:19.504185Z","shell.execute_reply":"2020-10-29T11:20:19.503533Z"},"papermill":{"duration":1.848096,"end_time":"2020-10-29T11:20:19.504318","exception":false,"start_time":"2020-10-29T11:20:17.656222","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Loading libraries needed for analysis\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom numpy.random import RandomState\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nsns.set_palette(['#06B1F0', '#FC4B60'])\nrandom_seed = 63445","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:19.580312Z","iopub.status.busy":"2020-10-29T11:20:19.579511Z","iopub.status.idle":"2020-10-29T11:20:19.618866Z","shell.execute_reply":"2020-10-29T11:20:19.618040Z"},"papermill":{"duration":0.08185,"end_time":"2020-10-29T11:20:19.619002","exception":false,"start_time":"2020-10-29T11:20:19.537152","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"new_data = pd.read_csv('./data_updated.csv', index_col=0)\nprint (\"DATA SHAPE: \", new_data.shape)\nnew_data.head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.033404,"end_time":"2020-10-29T11:20:19.686778","exception":false,"start_time":"2020-10-29T11:20:19.653374","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Class distribution"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:19.766603Z","iopub.status.busy":"2020-10-29T11:20:19.765487Z","iopub.status.idle":"2020-10-29T11:20:19.770653Z","shell.execute_reply":"2020-10-29T11:20:19.769845Z"},"papermill":{"duration":0.050119,"end_time":"2020-10-29T11:20:19.770779","exception":false,"start_time":"2020-10-29T11:20:19.720660","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"new_data['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.034094,"end_time":"2020-10-29T11:20:19.839339","exception":false,"start_time":"2020-10-29T11:20:19.805245","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The problem we are trying to solve in this analysis is breast cancer prediction. Based on the features available, we are going to predict whether the tumor a patient has is benign or malignant. One of the biggest challenges of this analysis is to deal with an imbalanced dataset"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:19.928348Z","iopub.status.busy":"2020-10-29T11:20:19.927396Z","iopub.status.idle":"2020-10-29T11:20:20.119599Z","shell.execute_reply":"2020-10-29T11:20:20.118743Z"},"papermill":{"duration":0.241268,"end_time":"2020-10-29T11:20:20.119738","exception":false,"start_time":"2020-10-29T11:20:19.878470","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"sns.countplot(new_data['Class'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.039499,"end_time":"2020-10-29T11:20:20.195927","exception":false,"start_time":"2020-10-29T11:20:20.156428","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Experiment setup"},{"metadata":{"papermill":{"duration":0.039909,"end_time":"2020-10-29T11:20:20.273634","exception":false,"start_time":"2020-10-29T11:20:20.233725","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Choosing the evaluation metrics is also quite challenging when dealing with imbalanced datasets. Accuracy is not the right metric as the accuracy of baseline model that classifies everything as over-represented class is 97%. Any model we build should beat this accuracy score.  "},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:20.361838Z","iopub.status.busy":"2020-10-29T11:20:20.360774Z","iopub.status.idle":"2020-10-29T11:20:20.364087Z","shell.execute_reply":"2020-10-29T11:20:20.363416Z"},"papermill":{"duration":0.050948,"end_time":"2020-10-29T11:20:20.364224","exception":false,"start_time":"2020-10-29T11:20:20.313276","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def metrics(true, preds):\n    \"\"\"\n    Function to calculate evaluation metrics \n    parameters: true values, predictions\n    prints accuracy, recall, precision and f1 scores\n    \"\"\"\n    accuracy = accuracy_score(true, preds)\n    recall = recall_score(true, preds)\n    precision = precision_score(true, preds)\n    f1score = f1_score(true, preds)\n    print ('accuracy: {}, recall: {}, precision: {}, f1-score: {}'.format(accuracy, recall, precision, f1score))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.038145,"end_time":"2020-10-29T11:20:20.438991","exception":false,"start_time":"2020-10-29T11:20:20.400846","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Baseline Model - Train and test without additional pre-processing"},{"metadata":{"papermill":{"duration":0.03615,"end_time":"2020-10-29T11:20:20.513825","exception":false,"start_time":"2020-10-29T11:20:20.477675","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Using stratified sampling instead of random train test split. This splits the target proportionally between training and test set."},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:20.595749Z","iopub.status.busy":"2020-10-29T11:20:20.594952Z","iopub.status.idle":"2020-10-29T11:20:20.629238Z","shell.execute_reply":"2020-10-29T11:20:20.628198Z"},"papermill":{"duration":0.079743,"end_time":"2020-10-29T11:20:20.629428","exception":false,"start_time":"2020-10-29T11:20:20.549685","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(new_data.drop('Class', axis=1), new_data['Class'], test_size = 0.30, random_state=random_seed, stratify=new_data['Class'])\nprint (\"TRAIN DATA SHAPE: \", x_train.shape)\nprint (\"TEST DATA SHAPE: \", x_test.shape)\nrf = RandomForestClassifier(n_estimators=5, random_state=random_seed)\nrf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.036,"end_time":"2020-10-29T11:20:20.703714","exception":false,"start_time":"2020-10-29T11:20:20.667714","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Applying to held-out set"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:20.787850Z","iopub.status.busy":"2020-10-29T11:20:20.786710Z","iopub.status.idle":"2020-10-29T11:20:20.795038Z","shell.execute_reply":"2020-10-29T11:20:20.794355Z"},"papermill":{"duration":0.055095,"end_time":"2020-10-29T11:20:20.795166","exception":false,"start_time":"2020-10-29T11:20:20.740071","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"preds = rf.predict(x_test)\nmetrics(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.037184,"end_time":"2020-10-29T11:20:20.870081","exception":false,"start_time":"2020-10-29T11:20:20.832897","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Over-sampling the minority class in training set using SMOTE"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:20.962512Z","iopub.status.busy":"2020-10-29T11:20:20.961735Z","iopub.status.idle":"2020-10-29T11:20:20.972579Z","shell.execute_reply":"2020-10-29T11:20:20.971825Z"},"papermill":{"duration":0.064512,"end_time":"2020-10-29T11:20:20.972709","exception":false,"start_time":"2020-10-29T11:20:20.908197","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"sm = SMOTE(random_state=random_seed)\nX, Y = sm.fit_sample(x_train, y_train, )\nprint ('Shape of oversampled data: {}'.format(X.shape))\nprint ('Shape of Y: {}'.format(Y.shape))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:21.065437Z","iopub.status.busy":"2020-10-29T11:20:21.064273Z","iopub.status.idle":"2020-10-29T11:20:21.068018Z","shell.execute_reply":"2020-10-29T11:20:21.067410Z"},"papermill":{"duration":0.053581,"end_time":"2020-10-29T11:20:21.068145","exception":false,"start_time":"2020-10-29T11:20:21.014564","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"X = np.floor(X).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:21.153883Z","iopub.status.busy":"2020-10-29T11:20:21.151743Z","iopub.status.idle":"2020-10-29T11:20:21.298008Z","shell.execute_reply":"2020-10-29T11:20:21.297356Z"},"papermill":{"duration":0.191248,"end_time":"2020-10-29T11:20:21.298131","exception":false,"start_time":"2020-10-29T11:20:21.106883","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"sns.countplot(Y)\nplt.title('Balanced training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:21.387802Z","iopub.status.busy":"2020-10-29T11:20:21.387024Z","iopub.status.idle":"2020-10-29T11:20:21.880112Z","shell.execute_reply":"2020-10-29T11:20:21.879023Z"},"papermill":{"duration":0.543439,"end_time":"2020-10-29T11:20:21.880253","exception":false,"start_time":"2020-10-29T11:20:21.336814","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=5, random_state=random_seed)\nrecall_scores = cross_val_score(rf, X, Y, scoring='recall', cv=5)\nf1_scores = cross_val_score(rf, X,Y, scoring='f1', cv=5)\naccuracy_scores = cross_val_score(rf, X,Y, scoring='accuracy', cv=5)\nprecision_scores = cross_val_score(rf, X,Y, scoring='precision', cv=5)\nprint ('Average Recall score: {}'.format(np.mean(recall_scores)))\nprint ('Average F1 scores: {}'.format(np.mean(f1_scores)))\nprint ('Average Accuracy scores: {}'.format(np.mean(accuracy_scores)))\nprint ('Average Precision scores: {}'.format(np.mean(precision_scores)))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.039966,"end_time":"2020-10-29T11:20:21.959820","exception":false,"start_time":"2020-10-29T11:20:21.919854","status":"completed"},"tags":[]},"cell_type":"markdown","source":"whoohooo. the cross validation scores look amazing. Let's see how it performs on test data"},{"metadata":{"papermill":{"duration":0.041799,"end_time":"2020-10-29T11:20:22.041663","exception":false,"start_time":"2020-10-29T11:20:21.999864","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Results for training set"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:22.131931Z","iopub.status.busy":"2020-10-29T11:20:22.131124Z","iopub.status.idle":"2020-10-29T11:20:22.239202Z","shell.execute_reply":"2020-10-29T11:20:22.238279Z"},"papermill":{"duration":0.157023,"end_time":"2020-10-29T11:20:22.239363","exception":false,"start_time":"2020-10-29T11:20:22.082340","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"preds = cross_val_predict(rf, X, Y, cv=5)\nprint ('Accuracy score: {}'.format(accuracy_score(Y, preds)))\nprint ('Recall score: {}'.format(recall_score(Y, preds)))\nprint ('Precision score: {}'.format(precision_score(Y, preds)))\nprint ('f1-score: {}'.format(f1_score(Y, preds)))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.040256,"end_time":"2020-10-29T11:20:22.322501","exception":false,"start_time":"2020-10-29T11:20:22.282245","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Results for test set"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:22.420070Z","iopub.status.busy":"2020-10-29T11:20:22.414931Z","iopub.status.idle":"2020-10-29T11:20:22.444313Z","shell.execute_reply":"2020-10-29T11:20:22.444999Z"},"papermill":{"duration":0.080936,"end_time":"2020-10-29T11:20:22.445159","exception":false,"start_time":"2020-10-29T11:20:22.364223","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=5, random_state=random_seed)\nrf.fit(X, Y)\ntest_preds = rf.predict(x_test)\nprint ('Accuracy score: {}'.format(accuracy_score(y_test, test_preds)))\nprint ('Recall score: {}'.format(recall_score(y_test, test_preds)))\nprint ('Precision score: {}'.format(precision_score(y_test, test_preds)))\nprint ('f1-score: {}'.format(f1_score(y_test, test_preds)))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.040236,"end_time":"2020-10-29T11:20:22.526562","exception":false,"start_time":"2020-10-29T11:20:22.486326","status":"completed"},"tags":[]},"cell_type":"markdown","source":"What happened? cross-validated score is almost 100% but test recall is only 50%. looks like model is over-fitting. But we did cross-validation to detect if our model was overfitting"},{"metadata":{"papermill":{"duration":0.040268,"end_time":"2020-10-29T11:20:22.607806","exception":false,"start_time":"2020-10-29T11:20:22.567538","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Let's look at the distribution of features over 2 classes. "},{"metadata":{"papermill":{"duration":0.040736,"end_time":"2020-10-29T11:20:22.690296","exception":false,"start_time":"2020-10-29T11:20:22.649560","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Distribution of features over malignant and benign tumors"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:22.791597Z","iopub.status.busy":"2020-10-29T11:20:22.782024Z","iopub.status.idle":"2020-10-29T11:20:29.470575Z","shell.execute_reply":"2020-10-29T11:20:29.471155Z"},"papermill":{"duration":6.739833,"end_time":"2020-10-29T11:20:29.471307","exception":false,"start_time":"2020-10-29T11:20:22.731474","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"ccols = new_data.columns\n\ngrouped_data = new_data.groupby('Class')\nsns.set(font_scale=1.8)\nsns.set_palette(['#06B1F0', '#FC4B60'])\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 16), \n                         tight_layout=True)\nfor ax, p in zip(axes.ravel(), ccols):\n    for k, v in grouped_data[p]:\n        sns.kdeplot(v, ax=ax, label=str(k)+\":\"+v.name)\n#         plt.setp(ax.get_legend().get_texts(), fontsize='22')\nplt.savefig('feature_distributions.png')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.046131,"end_time":"2020-10-29T11:20:29.564837","exception":false,"start_time":"2020-10-29T11:20:29.518706","status":"completed"},"tags":[]},"cell_type":"markdown","source":"In the kernel density plots we are looking for variables with very little overlap between the malignant and benign tumor. Out of nine features, only two features have high discriminatory power between class 0 and 1"},{"metadata":{"papermill":{"duration":0.045816,"end_time":"2020-10-29T11:20:29.657676","exception":false,"start_time":"2020-10-29T11:20:29.611860","status":"completed"},"tags":[]},"cell_type":"markdown","source":"so the features did not contribute to model overfitting. It could be that there is something wrong with the way the oversampling and cross-validation were performed"},{"metadata":{"papermill":{"duration":0.046693,"end_time":"2020-10-29T11:20:29.751958","exception":false,"start_time":"2020-10-29T11:20:29.705265","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Looks like there was information bleed from the validation set to the training set.\n\n![smote_before_cross_val](https://www.marcoaltini.com/uploads/1/3/2/3/13234002/2639934.jpg?401)\n\nHere the issue is that the synthetic observations end up in both the training and validation sets during the same iteration. Models like Random Forest are able to recognise that these values are from the same distribution and the validation set predictions become very accurate.\n\nMoreover, another intuitive issue is that during validation, the model is being both trained and validated on a **balanced** dataset, meaning both the train and validation sets are balanced. However, when we do ahead to the testing phase, we are essentially using a model trained on balanced dataset and testing it on an unbalanced dataset. Therefore the validation step, which is supposed to be representative of the testing environment no longer serves that purpose.\n\nThe ideal way would be to perform oversampling of the training data in each cross-validation iteration. This would prevent the data leakage from the validation set to the training set during cross-validation.\n\n![smote with cross val](https://www.marcoaltini.com/uploads/1/3/2/3/13234002/9101820.jpg?372)"},{"metadata":{"papermill":{"duration":0.050232,"end_time":"2020-10-29T11:20:29.848648","exception":false,"start_time":"2020-10-29T11:20:29.798416","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Oversampling in each cross-validation loop"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:29.962404Z","iopub.status.busy":"2020-10-29T11:20:29.957222Z","iopub.status.idle":"2020-10-29T11:20:30.145425Z","shell.execute_reply":"2020-10-29T11:20:30.144348Z"},"papermill":{"duration":0.248471,"end_time":"2020-10-29T11:20:30.145747","exception":false,"start_time":"2020-10-29T11:20:29.897276","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"kf = StratifiedKFold(n_splits=5, random_state=random_seed)\ncross_val_f1_score_lst = []\ncross_val_accuracy_lst = []\ncross_val_recall_lst = []\ncross_val_precision_lst = []\n\nfor train_index_ls, validation_index_ls in kf.split(x_train, y_train):\n    # keeping validation set apart and oversampling in each iteration using smote \n    train, validation = x_train.iloc[train_index_ls], x_train.iloc[validation_index_ls]\n    target_train, target_val = y_train.iloc[train_index_ls], y_train.iloc[validation_index_ls]\n    sm = SMOTE(random_state=random_seed)\n    X_train_res, y_train_res = sm.fit_sample(train, target_train)\n    print (X_train_res.shape, y_train_res.shape)\n    \n    # training the model on oversampled 4 folds of training set\n    rf = RandomForestClassifier(n_estimators=5, random_state=random_seed)\n    rf.fit(X_train_res, y_train_res)\n    # testing on 1 fold of validation set\n    validation_preds = rf.predict(validation)\n    cross_val_recall_lst.append(recall_score(target_val, validation_preds))\n    cross_val_accuracy_lst.append(accuracy_score(target_val, validation_preds))\n    cross_val_precision_lst.append(precision_score(target_val, validation_preds))\n    cross_val_f1_score_lst.append(f1_score(target_val, validation_preds))\nprint ('Cross validated accuracy: {}'.format(np.mean(cross_val_accuracy_lst)))\nprint ('Cross validated recall score: {}'.format(np.mean(cross_val_recall_lst)))\nprint ('Cross validated precision score: {}'.format(np.mean(cross_val_precision_lst)))\nprint ('Cross validated f1_score: {}'.format(np.mean(cross_val_f1_score_lst)))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"papermill":{"duration":0.050499,"end_time":"2020-10-29T11:20:30.248427","exception":false,"start_time":"2020-10-29T11:20:30.197928","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### By doing over-sampling under each cross-validation loop, the cross-validated scores are representative of test set scores above "},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:30.363902Z","iopub.status.busy":"2020-10-29T11:20:30.363049Z","iopub.status.idle":"2020-10-29T11:20:30.372812Z","shell.execute_reply":"2020-10-29T11:20:30.371981Z"},"papermill":{"duration":0.07058,"end_time":"2020-10-29T11:20:30.372949","exception":false,"start_time":"2020-10-29T11:20:30.302369","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"r_s, p_s = np.mean(recall_scores), np.mean(precision_scores)\na_s, f_s = np.mean(accuracy_scores), np.mean(f1_scores)\nr_s_1, p_s_1 = np.mean(cross_val_recall_lst), np.mean(cross_val_precision_lst)\na_s_1, f_s_1 = np.mean(cross_val_accuracy_lst), np.mean(cross_val_f1_score_lst)\nmetrics_df_wrong = pd.DataFrame(list(zip([r_s], [p_s], [f_s], [a_s])), columns=['Recall', 'Precision', 'F1-score', 'Accuracy'])\nmetrics_df_right = pd.DataFrame(list(zip([r_s_1], [p_s_1], [f_s_1], [a_s_1])), columns=['Recall', 'Precision', 'F1-score', 'Accuracy'])\nmetrics_df = pd.concat([metrics_df_wrong, metrics_df_right], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:30.476421Z","iopub.status.busy":"2020-10-29T11:20:30.475245Z","iopub.status.idle":"2020-10-29T11:20:30.478726Z","shell.execute_reply":"2020-10-29T11:20:30.478119Z"},"papermill":{"duration":0.057358,"end_time":"2020-10-29T11:20:30.478854","exception":false,"start_time":"2020-10-29T11:20:30.421496","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"metrics_df.index = (['Cross validation and oversampling done wrong', 'Cross validation and oversampling done right'])\nmetrics_df = metrics_df.transpose()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-29T11:20:30.591253Z","iopub.status.busy":"2020-10-29T11:20:30.590170Z","iopub.status.idle":"2020-10-29T11:20:30.826718Z","shell.execute_reply":"2020-10-29T11:20:30.825822Z"},"papermill":{"duration":0.301177,"end_time":"2020-10-29T11:20:30.826861","exception":false,"start_time":"2020-10-29T11:20:30.525684","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nax = plt.subplot(111)\nax.bar([0,2,4,6], metrics_df['Cross validation and oversampling done wrong'], color='deepskyblue', label='Cross validation and oversampling done wrong')\nax.bar([0.8,2.8,4.8,6.8], metrics_df['Cross validation and oversampling done right'], color = 'red', label = 'Cross validation and oversampling done right')\nax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.2))\nplt.xticks([0,2,4,6], ['Recall', 'Precision', 'F1-score', 'Accuracy'], horizontalalignment='left')\nplt.xlabel('Metrics')\nplt.ylabel('Cross-validated score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.05078,"end_time":"2020-10-29T11:20:30.931245","exception":false,"start_time":"2020-10-29T11:20:30.880465","status":"completed"},"tags":[]},"cell_type":"markdown","source":"As we can see from the plot above, the performance of the model has improved to give 50% recall score after oversampling and more representative results for test set were obtained when cross-validation and oversampling was done right."},{"metadata":{},"cell_type":"markdown","source":"## Balanced Ensembling Methods\n\n#### Methods generating under-sampled subsets combined inside an ensemble.\nFor various algorithms: https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.ensemble "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(new_data.drop('Class', axis=1), new_data['Class'], test_size = 0.30, random_state=random_seed, stratify=new_data['Class'])\nprint (\"TRAIN DATA SHAPE: \", x_train.shape)\nprint (\"TEST DATA SHAPE: \", x_test.shape)\nerf = EasyEnsembleClassifier( n_estimators=100, base_estimator= RandomForestClassifier(n_estimators=5, random_state=random_seed))\nerf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = erf.predict(x_test)\nmetrics(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For default combination of under and oversampling techniques: https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.combine"},{"metadata":{},"cell_type":"markdown","source":"### MCC, AuPRC metrics metrics look at not just the accuracy scores but also F1, precision, recall which gives us an overall view of the model performance."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}